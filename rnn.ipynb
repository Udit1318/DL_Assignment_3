{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0d6fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_MODE=disabled\n"
     ]
    }
   ],
   "source": [
    "import requests,zipfile,io\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import warnings\n",
    "import wandb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import argparse\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "device = 'cuda:0'\n",
    "# %env WANDB_MODE=disabled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ab54e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus(data):\n",
    "    eng_corpus = set()  # Set to store English characters\n",
    "    hin_corpus = set()  # Set to store Hindi characters\n",
    "\n",
    "    for eng_word, hin_word in zip(data[1], data[0]):\n",
    "        # Check if both are strings before proceeding\n",
    "        if isinstance(eng_word, str):\n",
    "            eng_corpus.update(eng_word)\n",
    "        else:\n",
    "            print(f\"Skipping non-string eng_word: {eng_word} (type: {type(eng_word)})\")\n",
    "\n",
    "        if isinstance(hin_word, str):\n",
    "            hin_corpus.update(hin_word)\n",
    "        else:\n",
    "            print(f\"Skipping non-string hin_word: {hin_word} (type: {type(hin_word)})\")\n",
    "\n",
    "    # Add end delimiter characters to both corpora\n",
    "    eng_corpus.add('#')\n",
    "    hin_corpus.add('#')\n",
    "    hin_corpus.add('$')\n",
    "    eng_corpus.add('$')\n",
    "\n",
    "    # Add start delimiter character to the Hindi corpus\n",
    "    hin_corpus.add('^')\n",
    "\n",
    "    return hin_corpus, eng_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5cb76d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2index(data):\n",
    "    hin_corpus, eng_corpus = get_corpus(data)  # Get Hindi and English corpora from data\n",
    "\n",
    "    engchar_idx = {}  # Dictionary to map English characters to indices\n",
    "    hinchar_idx = {}  # Dictionary to map Hindi characters to indices\n",
    "    idx_engchar = {}  # Dictionary to map indices to English characters\n",
    "    idx_hinchar = {}  # Dictionary to map indices to Hindi characters\n",
    "\n",
    "    # Assign indices to English characters and vice versa\n",
    "    for i, char in enumerate(eng_corpus):\n",
    "        engchar_idx[char] = i\n",
    "        idx_engchar[i] = char\n",
    "\n",
    "    # Assign indices to Hindi characters and vice versa\n",
    "    for i, char in enumerate(hin_corpus):\n",
    "        hinchar_idx[char] = i\n",
    "        idx_hinchar[i] = char\n",
    "\n",
    "    eng_vocab_size = len(eng_corpus)  # Vocabulary size of English corpus\n",
    "    hin_vocab_size = len(hin_corpus)  # Vocabulary size of Hindi corpus\n",
    "\n",
    "    return engchar_idx, hinchar_idx, idx_engchar, idx_hinchar, eng_vocab_size, hin_vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Read the train, test, and validation datasets from TSV files\n",
    "    tpath = \"/kaggle/input/dataset-mine/dataset/DakshinaDataSet_Hindi/hindi_Train_dataset.csv\"\n",
    "    vpath = \"/kaggle/input/dataset-mine/dataset/DakshinaDataSet_Hindi/hindi_Validation_dataset.csv\"\n",
    "    tspath = \"/kaggle/input/dataset-mine/dataset/DakshinaDataSet_Hindi/hindi_Test_dataset.csv\"\n",
    "    train_df = pd.read_csv(tpath, sep='\\t', header=None, dtype=str, \n",
    "        keep_default_na=False)\n",
    "    val_df = pd.read_csv(vpath, sep='\\t', header=None, dtype=str,\n",
    "        keep_default_na=False)\n",
    "    test_df = pd.read_csv(tspath, sep='\\t', header=None, dtype=str,\n",
    "        keep_default_na=False)\n",
    "\n",
    "    train_df = train_df.drop(columns=[2])\n",
    "    val_df = val_df.drop(columns=[2])\n",
    "    test_df = test_df.drop(columns=[2])\n",
    "\n",
    "    # train_df = train_df.dropna()\n",
    "    # val_df = val_df.dropna()\n",
    "    # test_df = test_df.dropna()\n",
    "\n",
    "    # Convert words to indices and retrieve vocabulary information\n",
    "    eng_to_idx, hin_to_idx, idx_to_eng, idx_to_hin, input_len, target_len = word2index(train_df)\n",
    "\n",
    "    # Return the datasets and vocabulary information\n",
    "    return train_df, test_df, val_df, eng_to_idx, hin_to_idx, idx_to_eng, idx_to_hin, input_len, target_len\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff978f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def maxlen(data):\n",
    "    maxlen_eng = max(len(word) for word in data[1])  # Maximum length of English words\n",
    "    maxlen_hin = max(len(word) for word in data[0])  # Maximum length of Hindi words\n",
    "    return maxlen_eng, maxlen_hin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4516daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(data, eng_to_idx, hin_to_idx):\n",
    "    eng = []  # List to store pre-processed English sentences\n",
    "    hin = []  # List to store pre-processed Hindi sentences\n",
    "\n",
    "    maxlen_eng, maxlen_hin = maxlen(data)  # Get the maximum lengths of English and Hindi words\n",
    "\n",
    "    unknown = eng_to_idx['$']  # Index for unknown character in English corpus\n",
    "    print(len(data))\n",
    "    for i in range(0, len(data)):\n",
    "        sz = 0  # Variable to track the size of the sentence\n",
    "        eng_word = data[1][i]  # English word at index i\n",
    "        hin_word = '^' + data[0][i]  # Add start delimiter (^) to Hindi word\n",
    "\n",
    "        # Pad the English and Hindi words to their respective maximum lengths\n",
    "        eng_word = eng_word.ljust(maxlen_eng + 1, '#')\n",
    "        hin_word = hin_word.ljust(maxlen_hin + 1, '#')\n",
    "\n",
    "        idx = []\n",
    "        for char in eng_word:\n",
    "            if eng_to_idx.get(char) is not None:\n",
    "                idx.append(eng_to_idx[char])  # Append the index of the character if it exists in the corpus\n",
    "            else:\n",
    "                idx.append(unknown)  # Append the index of unknown character otherwise\n",
    "        eng.append(idx)\n",
    "\n",
    "        idx = []\n",
    "        for char in hin_word:\n",
    "            if hin_to_idx.get(char) is not None:\n",
    "                idx.append(hin_to_idx[char])  # Append the index of the character if it exists in the corpus\n",
    "            else:\n",
    "                idx.append(unknown)  # Append the index of unknown character otherwise\n",
    "        hin.append(idx)\n",
    "\n",
    "    return eng, hin\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5867fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, y):\n",
    "    count = 0\n",
    "    for p, target in zip(predictions, y):\n",
    "        if np.array_equal(p, target):\n",
    "            count += 1\n",
    "    return (count / len(predictions)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0a2ebc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, train_x, train_y, transform=None):\n",
    "        self.train_x = train_x  # Input data (train_x)\n",
    "        self.train_y = train_y  # Target data (train_y)\n",
    "        self.transform = transform  # Optional data transformation\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_x)  # Return the length of the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)  # Apply the transformation (if any) to the sample\n",
    "\n",
    "        # Return the input and target data tensors for the given index\n",
    "        return torch.tensor(self.train_x[idx]).to(device), torch.tensor(self.train_y[idx]).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d73c3281",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, embedding_size, num_layers, batch_size, bidirectional, dropout_p=0.1):\n",
    "        super(EncoderLSTM, self).__init__()\n",
    "        # Initialize the hidden size, batch size, embedding size, number of layers, and bidirectional flag\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = bidirectional == \"Yes\"\n",
    "\n",
    "        # Create an embedding layer to convert input tokens to dense vectors\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        \n",
    "        # Create an LSTM layer with the specified parameters\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p, bidirectional=self.bidirectional)\n",
    "        \n",
    "        # Create a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden, state):\n",
    "        # Pass the input through the embedding layer and apply dropout\n",
    "        embedded = self.dropout(self.embedding(input).view(-1, self.batch_size, self.embedding_size))\n",
    "        \n",
    "        # Pass the embedded input and the hidden state to the LSTM layer\n",
    "        output, (hidden, state) = self.lstm(embedded, (hidden, state))\n",
    "\n",
    "        # If the LSTM is bidirectional, adjust the hidden and state tensors\n",
    "        if self.bidirectional:\n",
    "            # Reshape the hidden and state tensors to separate the directions\n",
    "            hidden = hidden.view(2, self.num_layers, self.batch_size, self.hidden_size)\n",
    "            state = state.view(2, self.num_layers, self.batch_size, self.hidden_size)\n",
    "            # Average the hidden and state tensors from both directions\n",
    "            hidden = (hidden[0] + hidden[1]) / 2\n",
    "            state = (state[0] + state[1]) / 2\n",
    "\n",
    "        return output, hidden, state\n",
    "\n",
    "    def initHidden(self):\n",
    "        # Determine the number of layers based on whether the LSTM is bidirectional\n",
    "        layers = 2 * self.num_layers if self.bidirectional else self.num_layers\n",
    "        # Initialize the hidden state with zeros\n",
    "        return torch.zeros(layers, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "    def initState(self):\n",
    "        # Determine the number of layers based on whether the LSTM is bidirectional\n",
    "        layers = 2 * self.num_layers if self.bidirectional else self.num_layers\n",
    "        # Initialize the cell state with zeros\n",
    "        return torch.zeros(layers, self.batch_size, self.hidden_size, device=device)\n",
    "\n",
    "\n",
    "class DecoderLSTM(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, embedding_size, num_layers, batch_size, dropout_p=0.1):\n",
    "        super(DecoderLSTM, self).__init__()\n",
    "        # Initialize hidden size, embedding size, number of layers, and batch size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_layers = num_layers\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Create an embedding layer for output tokens\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        \n",
    "        # Create an LSTM layer with the specified parameters\n",
    "        self.lstm = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=dropout_p)\n",
    "        \n",
    "        # Create a linear layer to map the LSTM output to the target vocabulary size\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Apply log softmax activation function along the specified dimension\n",
    "        self.softmax = nn.LogSoftmax(dim=2)\n",
    "        \n",
    "        # Initialize the dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input, hidden, state):\n",
    "        # Pass the input through the embedding layer and reshape\n",
    "        embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n",
    "        \n",
    "        # (Optional) Apply dropout to the embedded input (commented out in this version)\n",
    "        # embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Pass the embedded input, hidden state, and cell state to the LSTM layer\n",
    "        output, (hidden, state) = self.lstm(embedded, (hidden, state))\n",
    "        \n",
    "        # Pass the LSTM output through the linear layer and apply softmax\n",
    "        output = self.softmax(self.out(output))\n",
    "        \n",
    "        return output, hidden, state\n",
    "\n",
    "\n",
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, output_size, hidden_size, embedding_size, decoder_layers, batch_size, rnn_type, dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        # Initialize attributes including hidden size, output size, dropout rate, batch size, RNN type, embedding size, and number of decoder layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.batch_size = batch_size\n",
    "        self.rnn_type = rnn_type\n",
    "        self.embedding_size = embedding_size\n",
    "        self.decoder_layers = decoder_layers\n",
    "\n",
    "        # Create an embedding layer for output tokens\n",
    "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
    "        \n",
    "        # Create a dropout layer to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "        # Attention mechanism linear layers\n",
    "        self.attn_U = nn.Linear(hidden_size, hidden_size, bias=False).to(device)\n",
    "        self.attn_W = nn.Linear(hidden_size, hidden_size, bias=False).to(device)\n",
    "        self.attn_V = nn.Linear(hidden_size, 1, bias=False).to(device)\n",
    "\n",
    "        # Output linear layer\n",
    "        self.output_linear = nn.Linear(hidden_size, output_size, bias=True)\n",
    "        \n",
    "        # Softmax activation functions\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.log_softmax = nn.LogSoftmax(dim=2)\n",
    "\n",
    "        # Determine the input size for the RNN based on the embedding size and hidden size\n",
    "        rnn_input_size = embedding_size + hidden_size\n",
    "\n",
    "        # Choose the appropriate RNN type based on the provided argument\n",
    "        if rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(rnn_input_size, hidden_size, decoder_layers, dropout=dropout_p)\n",
    "        elif rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(rnn_input_size, hidden_size, decoder_layers, dropout=dropout_p)\n",
    "        elif rnn_type == \"RNN\":\n",
    "            self.rnn = nn.RNN(rnn_input_size, hidden_size, decoder_layers, dropout=dropout_p)\n",
    "\n",
    "\n",
    "def forward(self, input, hidden, encoder_outputs, seq_len, state=None):\n",
    "    # Embed the input and reshape it to match the expected input shape\n",
    "    embedded = self.embedding(input).view(-1, self.batch_size, self.embedding_size)\n",
    "\n",
    "    # Compute attention weights using the Bahdanau attention mechanism\n",
    "    temp1 = self.attn_W(hidden[-1])\n",
    "    temp2 = self.attn_U(encoder_outputs)\n",
    "    c = torch.zeros(self.batch_size, 1, self.hidden_size).to(device)\n",
    "    temp1 = temp1.unsqueeze(0)\n",
    "    e_j = self.attn_V(F.tanh(temp1 + temp2))\n",
    "    alpha_j = self.softmax(e_j)\n",
    "\n",
    "    # Compute the context vector using the attention weights and encoder outputs\n",
    "    c = torch.bmm(alpha_j.permute(1, 2, 0), encoder_outputs.permute(1, 0, 2))\n",
    "\n",
    "    # Concatenate the embedded input and context vector\n",
    "    combined_input = torch.cat((embedded[0], c.squeeze(1)), 1).unsqueeze(0)\n",
    "    combined_input = F.relu(combined_input)\n",
    "\n",
    "    # Pass the combined input through the RNN layer\n",
    "    if self.rnn_type == \"GRU\" or self.rnn_type == \"RNN\":\n",
    "        output, hidden = self.rnn(combined_input, hidden)\n",
    "    elif self.rnn_type == \"LSTM\":\n",
    "        output, (hidden, state) = self.rnn(combined_input, (hidden, state))\n",
    "\n",
    "    # Pass the RNN output through the linear layer and apply log softmax\n",
    "    output = self.log_softmax(self.output_linear(output))\n",
    "\n",
    "    # Return the output, updated hidden state (and cell state for LSTM), and attention weights\n",
    "    if self.rnn_type in [\"GRU\", \"RNN\"]:\n",
    "        return output, hidden, alpha_j\n",
    "    elif self.rnn_type == \"LSTM\":\n",
    "        return output, hidden, state, alpha_j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b1eb048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(training_data, encoder, decoder, loss_function, encoder_optimizer, decoder_optimizer, num_encoder_layers, num_decoder_layers, batch_sz, hidden_dim, bidirectional, cell_type, use_attention):\n",
    "    total_loss = 0\n",
    "    teacher_forcing_ratio = 0.5\n",
    "\n",
    "    for i, (input_seq, target_seq) in enumerate(training_data):\n",
    "        loss = 0\n",
    "        # Zero the gradients of both optimizers\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        input_seq = input_seq.T\n",
    "        target_seq = target_seq.T\n",
    "        time_steps = len(input_seq)\n",
    "\n",
    "        if cell_type in ['GRU', 'RNN']:\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            # Initialize encoder hidden state\n",
    "            encoder_output, encoder_hidden = encoder(input_seq, encoder_hidden)\n",
    "            # Prepare decoder hidden state based on the number of layers\n",
    "            if num_decoder_layers > num_encoder_layers:\n",
    "                decoder_hidden = encoder_hidden\n",
    "                while num_decoder_layers > num_encoder_layers:\n",
    "                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                    num_decoder_layers -= 1\n",
    "            elif num_decoder_layers < num_encoder_layers:\n",
    "                decoder_hidden = encoder_hidden[-num_decoder_layers:]\n",
    "            else:\n",
    "                decoder_hidden = encoder_hidden\n",
    "\n",
    "            # Set initial decoder input as the first token of the target sequence\n",
    "            decoder_input = target_seq[0]\n",
    "\n",
    "            if bidirectional.lower() == \"yes\":\n",
    "                split_tensor = torch.split(encoder_output, hidden_dim, dim=-1)\n",
    "                encoder_output = (split_tensor[0] + split_tensor[1]) / 2\n",
    "            # Determine if teacher forcing should be used\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "                for i in range(len(target_seq)):\n",
    "                    # Perform decoding with or without attention based on the parameter\n",
    "                    if use_attention.lower() == \"yes\":\n",
    "                        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, time_steps)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = target_seq[i]\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = target_seq[i]\n",
    "            else:\n",
    "                for i in range(len(target_seq)):\n",
    "                    if use_attention.lower() == \"yes\":\n",
    "                        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, time_steps)\n",
    "                        top_value, top_index = decoder_output.topk(1)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = top_index.squeeze().detach()\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                        top_value, top_index = decoder_output.topk(1)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = top_index.squeeze().detach()\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        elif cell_type == 'LSTM':\n",
    "            encoder_hidden = encoder.initHidden()\n",
    "            encoder_state = encoder.initState()\n",
    "            encoder_output, encoder_hidden, encoder_state = encoder(input_seq, encoder_hidden, encoder_state)\n",
    "\n",
    "            if num_decoder_layers > num_encoder_layers:\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_state = encoder_state\n",
    "                while num_decoder_layers > num_encoder_layers:\n",
    "                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                    decoder_state = torch.cat([decoder_state, encoder_state[-1].unsqueeze(0)], dim=0)\n",
    "                    num_decoder_layers -= 1\n",
    "            elif num_decoder_layers < num_encoder_layers:\n",
    "                decoder_hidden = encoder_hidden[-num_decoder_layers:]\n",
    "                decoder_state = encoder_state[-num_decoder_layers:]\n",
    "            else:\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_state = encoder_state\n",
    "\n",
    "            if bidirectional.lower() == \"yes\":\n",
    "                split_tensor = torch.split(encoder_output, hidden_dim, dim=-1)\n",
    "                encoder_output = (split_tensor[0] + split_tensor[1]) / 2\n",
    "\n",
    "            decoder_input = target_seq[0]\n",
    "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            if use_teacher_forcing:\n",
    "                for i in range(len(target_seq)):\n",
    "                    if use_attention.lower() == \"yes\":\n",
    "                        decoder_output, decoder_hidden, decoder_state, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, time_steps, decoder_state)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = target_seq[i]\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden, decoder_state = decoder(decoder_input, decoder_hidden, decoder_state)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = target_seq[i]\n",
    "            else:\n",
    "                for i in range(len(target_seq)):\n",
    "                    if use_attention.lower() == \"yes\":\n",
    "                        decoder_output, decoder_hidden, decoder_state, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, time_steps, decoder_state)\n",
    "                        top_value, top_index = decoder_output.topk(1)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = top_index.squeeze().detach()\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden, decoder_state = decoder(decoder_input, decoder_hidden, decoder_state)\n",
    "                        top_value, top_index = decoder_output.topk(1)\n",
    "                        loss += loss_function(decoder_output.squeeze(), target_seq[i])\n",
    "                        decoder_input = top_index.squeeze().detach()\n",
    "\n",
    "            loss.backward()\n",
    "            encoder_optimizer.step()\n",
    "            decoder_optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(training_data), encoder, decoder\n",
    "\n",
    "\n",
    "def train_iter(train_data, val_data, val_targets, input_dim, target_dim, epochs, batch_size, embed_dim, enc_layers, dec_layers, hidden_dim, rnn_type, bidirectional, dropout, use_attention, beam_size=0):\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    # Initialize the encoder and decoder based on the RNN type and attention mechanism\n",
    "    if rnn_type == 'GRU':\n",
    "        encoder = EncoderGRU(input_dim, hidden_dim, embed_dim, enc_layers, batch_size, bidirectional, dropout).to(device)\n",
    "        if use_attention == \"Yes\":\n",
    "            decoder = AttnDecoder(target_dim, hidden_dim, embed_dim, dec_layers, batch_size, rnn_type, dropout).to(device)\n",
    "        else:\n",
    "            decoder = DecoderGRU(target_dim, hidden_dim, embed_dim, dec_layers, batch_size, dropout).to(device)\n",
    "    elif rnn_type == 'RNN':\n",
    "        encoder = EncoderRNN(input_dim, hidden_dim, embed_dim, enc_layers, batch_size, bidirectional, dropout).to(device)\n",
    "        if use_attention == \"Yes\":\n",
    "            decoder = AttnDecoder(target_dim, hidden_dim, embed_dim, dec_layers, batch_size, rnn_type, dropout).to(device)\n",
    "        else:\n",
    "            decoder = DecoderRNN(target_dim, hidden_dim, embed_dim, dec_layers, batch_size, dropout).to(device)\n",
    "    elif rnn_type == 'LSTM':\n",
    "        print('Entered LSTM')\n",
    "        encoder = EncoderLSTM(input_dim, hidden_dim, embed_dim, enc_layers, batch_size, bidirectional, dropout).to(device)\n",
    "        if use_attention == \"Yes\":\n",
    "            decoder = AttnDecoder(target_dim, hidden_dim, embed_dim, dec_layers, batch_size, rnn_type, dropout).to(device)\n",
    "        else:\n",
    "            decoder = DecoderLSTM(target_dim, hidden_dim, embed_dim, dec_layers, batch_size, dropout).to(device)\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, encoder, decoder = train(train_data, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, enc_layers, dec_layers, batch_size, hidden_dim, bidirectional, rnn_type, use_attention)\n",
    "        val_outputs, val_loss, attn_weights = eval(val_data, encoder, decoder, enc_layers, dec_layers, batch_size, hidden_dim, bidirectional, rnn_type, use_attention)\n",
    "\n",
    "        train_losses.append(train_loss / len(train_data))\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        val_accuracy = accuracy(val_outputs, val_targets)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(f\"----- Epoch {epoch + 1} -----\")\n",
    "        print(f\"Train Loss: {train_loss / len(train_data)}\")\n",
    "        print(f\"Validation Loss: {val_loss}\")\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "\n",
    "    return train_losses, val_losses, val_accuracies, encoder, decoder, enc_layers, dec_layers\n",
    "\n",
    "def eval(input_data, encoder, decoder, encoder_layers, decoder_layers, batch_size, hidden_size, bi_directional, cell_type, attention, build_matrix=False):\n",
    "    with torch.no_grad():\n",
    "        loss_fun = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        total_loss = 0\n",
    "        pred_words = []\n",
    "        attention_matrix = []\n",
    "\n",
    "        for X, y in input_data:\n",
    "            attn = []\n",
    "            loss = 0\n",
    "            decoder_words = []\n",
    "            x = x.T\n",
    "            y = y.T\n",
    "\n",
    "            # Initialize the encoder hidden state\n",
    "            if cell_type == 'LSTM':\n",
    "                encoder_hidden = encoder.initHidden()\n",
    "                encoder_state = encoder.initState()\n",
    "                encoder_output, encoder_hidden, encoder_state = encoder(x, encoder_hidden, encoder_state)\n",
    "            else:\n",
    "                encoder_hidden = encoder.initHidden()\n",
    "                encoder_output, encoder_hidden = encoder(x, encoder_hidden)\n",
    "\n",
    "            timesteps = len(x)\n",
    "\n",
    "            if decoder_layers > encoder_layers:\n",
    "                i = decoder_layers\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_state = encoder_state if cell_type == 'LSTM' else None\n",
    "\n",
    "                while True:\n",
    "                    if i == encoder_layers:\n",
    "                        break\n",
    "                    decoder_hidden = torch.cat([decoder_hidden, encoder_hidden[-1].unsqueeze(0)], dim=0)\n",
    "                    if cell_type == 'LSTM':\n",
    "                        decoder_state = torch.cat([decoder_state, encoder_state[-1].unsqueeze(0)], dim=0)\n",
    "                    i -= 1\n",
    "            elif decoder_layers < encoder_layers:\n",
    "                decoder_hidden = encoder_hidden[-decoder_layers:]\n",
    "                decoder_state = encoder_state[-decoder_layers:] if cell_type == 'LSTM' else None\n",
    "            else:\n",
    "                decoder_hidden = encoder_hidden\n",
    "                decoder_state = encoder_state if cell_type == 'LSTM' else None\n",
    "\n",
    "            decoder_input = y[0]\n",
    "\n",
    "            if bi_directional == \"Yes\":\n",
    "                split_tensor = torch.split(encoder_output, hidden_size, dim=-1)\n",
    "                encoder_output = torch.add(split_tensor[0], split_tensor[1]) / 2\n",
    "\n",
    "            for i in range(len(y)):\n",
    "                if attention == \"Yes\":\n",
    "                    if cell_type == 'LSTM':\n",
    "                        decoder_output, decoder_hidden, decoder_state, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(x), decoder_state)\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden, attn_weights = decoder(decoder_input, decoder_hidden, encoder_output, len(x))\n",
    "                    max_prob, index = decoder_output.topk(1)\n",
    "                    loss += loss_fun(torch.squeeze(decoder_output), y[i])\n",
    "                    index = index.squeeze()\n",
    "                    decoder_input = index\n",
    "                    decoder_words.append(index.tolist())\n",
    "                    if build_matrix:\n",
    "                        attn.append(attn_weights)\n",
    "                else:\n",
    "                    if cell_type == 'LSTM':\n",
    "                        decoder_output, decoder_hidden, decoder_state = decoder(decoder_input, decoder_hidden, decoder_state)\n",
    "                    else:\n",
    "                        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
    "                    max_prob, index = decoder_output.topk(1)\n",
    "                    loss += loss_fun(torch.squeeze(decoder_output), y[i])\n",
    "                    index = index.squeeze()\n",
    "                    decoder_input = index\n",
    "                    # print(f'Index shape {index.shape}')\n",
    "                    decoder_words.append(index.tolist())\n",
    "\n",
    "            if build_matrix:\n",
    "                attention_matrix = torch.cat(attn, dim=2).to(device)\n",
    "\n",
    "            decoder_words = np.array(decoder_words)\n",
    "            pred_words.append(decoder_words.T)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        predictions = [word for batch in pred_words for word in batch]\n",
    "\n",
    "    return predictions, total_loss / (len(predictions) * len(predictions[0])), attention_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a950c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68217\n",
      "6864\n",
      "6827\n"
     ]
    }
   ],
   "source": [
    "train_df,test_df,val_df,eng_to_idx,hin_to_idx,idx_to_eng,idx_to_hin,input_len,target_len=get_data()\n",
    "\n",
    "train_x,train_y = pre_process(train_df,eng_to_idx,hin_to_idx)\n",
    "test_x,test_y = pre_process(test_df,eng_to_idx,hin_to_idx)\n",
    "val_x,val_y = pre_process(val_df,eng_to_idx,hin_to_idx)\n",
    "\n",
    "train_dataset=MyDataset(train_x,train_y)\n",
    "test_dataset=MyDataset(test_x,test_y)\n",
    "val_dataset=MyDataset(val_x,val_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ac4b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wandb_run_sweeps(train_dataset, val_dataset, test_dataset, train_y, val_y, test_y, input_len, target_len):\n",
    "    config = {\n",
    "        \"project\":\"joke\",\n",
    "        \"method\": 'grid',\n",
    "        \"metric\": {\n",
    "        'name': 'val_acc',\n",
    "        'goal': 'maximize'\n",
    "        },\n",
    "        'parameters' :{\n",
    "        \"epochs\": {\"values\":[5]},\n",
    "        \"batchsize\": {\"values\": [512]},\n",
    "        \"embedding_size\": {\"values\":[128]},\n",
    "        \"hidden_size\": {\"values\":[256]},\n",
    "        \"encoder_layers\": {\"values\":[2]},\n",
    "        \"decoder_layers\": {\"values\":[2]},\n",
    "        \"cell_type\": {\"values\":[\"LSTM\"]},\n",
    "        \"bi_directional\":{\"values\":[\"No\"]},\n",
    "        \"dropout\":{\"values\":[0.2, 0.3, 0.5]},\n",
    "        \"attention\":{\"values\":[\"No\"]},\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def train_rnn():\n",
    "        wandb.init()\n",
    "        name = '_CT_' + str(wandb.config.cell_type) + \"_BS_\" + str(wandb.config.batchsize) + \"_EPOCH_\" + str(\n",
    "            wandb.config.epochs) + \"_ES_\" + str(wandb.config.embedding_size) + \"_HS_\" + str(\n",
    "            wandb.config.hidden_size)\n",
    "        wandb.run.name = name\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=wandb.config.batchsize,drop_last=True)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=wandb.config.batchsize,drop_last=True)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=wandb.config.batchsize,drop_last=True)\n",
    "\n",
    "        print(f'Len train dataset = {len(train_dataset)}')\n",
    "        print(f'Len of train dataloader = {len(train_dataloader)}')\n",
    "\n",
    "        x = next(iter(train_dataloader))\n",
    "        print(x[0])\n",
    "        epoch_train_loss, epoch_val_loss, epoch_val_acc, encoder, decoder, encoder_layers, decoder_layers = train_iter(\n",
    "            train_dataloader, val_dataloader, val_y, input_len, target_len, wandb.config.epochs, wandb.config.batchsize,\n",
    "            wandb.config.embedding_size, wandb.config.encoder_layers, wandb.config.decoder_layers,\n",
    "            wandb.config.hidden_size, wandb.config.cell_type, wandb.config.bi_directional, wandb.config.dropout,\n",
    "            wandb.config.attention)\n",
    "\n",
    "        train_predictions, _, _ = eval(train_dataloader, encoder, decoder, wandb.config.encoder_layers,\n",
    "                                       wandb.config.decoder_layers, wandb.config.batchsize, wandb.config.hidden_size,\n",
    "                                       wandb.config.bi_directional, wandb.config.cell_type, wandb.config.attention)\n",
    "\n",
    "        test_predictions, _, _ = eval(test_dataloader, encoder, decoder, wandb.config.encoder_layers,\n",
    "                                      wandb.config.decoder_layers, wandb.config.batchsize, wandb.config.hidden_size,\n",
    "                                      wandb.config.bi_directional, wandb.config.cell_type, wandb.config.attention)\n",
    "\n",
    "        train_accuracy = accuracy(train_predictions, train_y)\n",
    "        test_accuracy = accuracy(test_predictions, test_y)\n",
    "        print(\"train_accuracy:\", train_accuracy)\n",
    "        print(\"test_accuracy:\", test_accuracy)\n",
    "\n",
    "        wandb.run.save()\n",
    "        wandb.run.finish()\n",
    "\n",
    "    wandb.login(key=\"\")\n",
    "    sweep_id = wandb.sweep(config, project=\"A3-DL\")\n",
    "    wandb.agent(sweep_id, function=train_rnn, count=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b24977e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: sj1kch3b\n",
      "Sweep URL: https://wandb.ai/cs23s011-sri-indian-institute-of-technology-madras/joke/sweeps/sj1kch3b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: b7tb9zrl with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tattention: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatchsize: 512\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbi_directional: No\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tcell_type: LSTM\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdecoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tdropout: 0.2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tembedding_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tencoder_layers: 2\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 256\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len train dataset = 68217\n",
      "Len of train dataloader = 133\n",
      "tensor([[ 4, 22, 26,  ...,  3,  3,  3],\n",
      "        [ 2,  0, 22,  ...,  3,  3,  3],\n",
      "        [ 2, 22, 17,  ...,  3,  3,  3],\n",
      "        ...,\n",
      "        [26, 21,  1,  ...,  3,  3,  3],\n",
      "        [26, 21,  8,  ...,  3,  3,  3],\n",
      "        [26, 21,  8,  ...,  3,  3,  3]], device='cuda:1')\n",
      "Entered LSTM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run b7tb9zrl errored:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hariguru/miniconda3/envs/env1/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "    self._function()\n",
      "  File \"/tmp/ipykernel_2340699/3302988572.py\", line 38, in train_rnn\n",
      "    epoch_train_loss, epoch_val_loss, epoch_val_acc, encoder, decoder, encoder_layers, decoder_layers = train_iter(\n",
      "                                                                                                        ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2340699/896659084.py\", line 159, in train_iter\n",
      "    val_outputs, val_loss, attn_weights = eval(val_data, encoder, decoder, enc_layers, dec_layers, batch_size, hidden_dim, bidirectional, rnn_type, use_attention)\n",
      "                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_2340699/896659084.py\", line 185, in eval\n",
      "    x = x.T\n",
      "        ^\n",
      "UnboundLocalError: cannot access local variable 'x' where it is not associated with a value\n",
      "\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run b7tb9zrl errored:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Traceback (most recent call last):\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/home/hariguru/miniconda3/envs/env1/lib/python3.11/site-packages/wandb/agents/pyagent.py\", line 308, in _run_job\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     self._function()\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_2340699/3302988572.py\", line 38, in train_rnn\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     epoch_train_loss, epoch_val_loss, epoch_val_acc, encoder, decoder, encoder_layers, decoder_layers = train_iter(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                                                                                         ^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_2340699/896659084.py\", line 159, in train_iter\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     val_outputs, val_loss, attn_weights = eval(val_data, encoder, decoder, enc_layers, dec_layers, batch_size, hidden_dim, bidirectional, rnn_type, use_attention)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m   File \"/tmp/ipykernel_2340699/896659084.py\", line 185, in eval\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m     x = x.T\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m         ^\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m UnboundLocalError: cannot access local variable 'x' where it is not associated with a value\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m \n"
     ]
    }
   ],
   "source": [
    "wandb_run_sweeps(train_dataset,val_dataset,test_dataset,train_y,val_y,test_y,input_len,target_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
